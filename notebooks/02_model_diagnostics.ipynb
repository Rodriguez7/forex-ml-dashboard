{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Diagnostics & Performance Analysis\n",
        "\n",
        "This notebook analyzes the current baseline model to identify:\n",
        "- Probability distribution vs true labels\n",
        "- Where the model has strong vs weak separation\n",
        "- Per-symbol performance breakdown\n",
        "- Feature importance and correlations\n",
        "- Opportunities for improvement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, confusion_matrix, classification_report\n",
        "\n",
        "from src.models import load_model\n",
        "from src.dataset import get_train_val_test_splits, load_labeled_dataset\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "model, feature_names, metadata = load_model(\"lgbm_baseline\")\n",
        "\n",
        "# Load datasets\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_train_val_test_splits()\n",
        "\n",
        "print(f\"Model: {len(feature_names)} features\")\n",
        "print(f\"Train: {len(X_train):,} samples\")\n",
        "print(f\"Val: {len(X_val):,} samples\")\n",
        "print(f\"Test: {len(X_test):,} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Probability Distribution Analysis\n",
        "\n",
        "Key insight: Is the model confident or uncertain in its predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
        "y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "# Plot probability distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Test set - all predictions\n",
        "ax = axes[0, 0]\n",
        "ax.hist(y_pred_proba_test, bins=50, alpha=0.7, edgecolor='black')\n",
        "ax.axvline(0.5, color='red', linestyle='--', label='Decision boundary')\n",
        "ax.set_xlabel('Predicted Probability (Long Win)')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Test Set: Probability Distribution (All Predictions)', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Test set - by actual outcome\n",
        "ax = axes[0, 1]\n",
        "ax.hist(y_pred_proba_test[y_test == 1], bins=50, alpha=0.6, label='Actual Long Wins', color='green')\n",
        "ax.hist(y_pred_proba_test[y_test == 0], bins=50, alpha=0.6, label='Actual Short Wins', color='red')\n",
        "ax.axvline(0.5, color='black', linestyle='--', alpha=0.5)\n",
        "ax.set_xlabel('Predicted Probability')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Test Set: Probabilities by True Label', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Calibration plot\n",
        "ax = axes[1, 0]\n",
        "bins = np.linspace(0, 1, 11)\n",
        "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "true_probs = []\n",
        "for i in range(len(bins)-1):\n",
        "    mask = (y_pred_proba_test >= bins[i]) & (y_pred_proba_test < bins[i+1])\n",
        "    if mask.sum() > 0:\n",
        "        true_probs.append(y_test[mask].mean())\n",
        "    else:\n",
        "        true_probs.append(np.nan)\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
        "ax.plot(bin_centers, true_probs, 'o-', linewidth=2, markersize=8, label='Model calibration')\n",
        "ax.set_xlabel('Predicted Probability')\n",
        "ax.set_ylabel('Actual Fraction of Long Wins')\n",
        "ax.set_title('Calibration Curve', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Win rate by confidence bucket\n",
        "ax = axes[1, 1]\n",
        "buckets = [(0, 0.4), (0.4, 0.45), (0.45, 0.5), (0.5, 0.55), (0.55, 0.6), (0.6, 1.0)]\n",
        "bucket_names = []\n",
        "win_rates = []\n",
        "counts = []\n",
        "\n",
        "for low, high in buckets:\n",
        "    mask = (y_pred_proba_test >= low) & (y_pred_proba_test < high)\n",
        "    if mask.sum() > 0:\n",
        "        bucket_names.append(f'{low:.2f}-{high:.2f}')\n",
        "        win_rates.append(y_test[mask].mean() * 100)\n",
        "        counts.append(mask.sum())\n",
        "\n",
        "x_pos = range(len(bucket_names))\n",
        "bars = ax.bar(x_pos, win_rates, alpha=0.7)\n",
        "ax.axhline(50, color='red', linestyle='--', alpha=0.5, label='Random (50%)')\n",
        "ax.set_xlabel('Probability Bucket')\n",
        "ax.set_ylabel('Win Rate (%)')\n",
        "ax.set_title('Win Rate by Confidence Bucket', fontweight='bold')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(bucket_names, rotation=45)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count labels\n",
        "for i, (bar, count) in enumerate(zip(bars, counts)):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "            f'n={count}', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY INSIGHTS:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Mean predicted probability: {y_pred_proba_test.mean():.3f}\")\n",
        "print(f\"Std predicted probability: {y_pred_proba_test.std():.3f}\")\n",
        "print(f\"% predictions near 0.5 (0.45-0.55): {((y_pred_proba_test >= 0.45) & (y_pred_proba_test <= 0.55)).mean()*100:.1f}%\")\n",
        "print(f\"% high confidence (>0.6 or <0.4): {((y_pred_proba_test > 0.6) | (y_pred_proba_test < 0.4)).mean()*100:.1f}%\")\n",
        "\n",
        "## 3. Feature Importance Analysis\n",
        "\n",
        "# Top 20 features\n",
        "importance_df = metadata['feature_importance'].head(20)\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(range(len(importance_df)), importance_df['importance'])\n",
        "plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Top 20 Most Important Features', fontweight='bold')\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
